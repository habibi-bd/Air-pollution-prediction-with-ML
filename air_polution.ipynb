{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg213SMkUiyM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/updated_pollution_dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "g5eXoIObU1K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum( )"
      ],
      "metadata": {
        "id": "zUYP8ifwjpCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "4Buhb5ZdkEsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Air Quality'].unique()"
      ],
      "metadata": {
        "id": "5nHTb6MnVbbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['Air Quality'] = label_encoder.fit_transform(df['Air Quality'])\n",
        "\n",
        "corr_matrix = df.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of DataFrame')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ucH1EW22U-GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PM2.5\n",
        "# PM10\n",
        "\n"
      ],
      "metadata": {
        "id": "CoM341dglVLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X = df.drop('Air Quality', axis=1)\n",
        "y= df['Air Quality']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "gnAl9_1HY7cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_scale=['Temperature', 'Humidity', 'PM2.5', 'PM10', 'NO2', 'SO2', 'CO',\n",
        "       'Proximity_to_Industrial_Areas', 'Population_Density']\n",
        "scaler=StandardScaler()\n",
        "\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "GIduHrXlYjni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr=LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_scaled,y_train)\n",
        "y_pred=lr.predict(X_test_scaled)\n",
        "accuracy_score(y_test,y_pred)\n",
        "# output 0.947 before hyperparamter tune\n"
      ],
      "metadata": {
        "id": "MQKBie3_ZbdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Best parameters: {'C': 10}\n",
        "# Best cross-validation accuracy: 0.9427499999999999"
      ],
      "metadata": {
        "id": "YKpddICKg_k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_lr = grid_search.best_estimator_\n",
        "y_pred_tuned = best_lr.predict(X_test_scaled)\n",
        "test_accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print(\"Test accuracy with tuned model:\", test_accuracy_tuned)\n",
        "train_accuracy= best_lr.predict(X_train_scaled)\n",
        "print(\"Train accuracy with tuned model:\", accuracy_score(y_train, train_accuracy))\n",
        "\n",
        "# Test accuracy with tuned model: 0.942\n",
        "# Train accuracy with tuned model: 0.945"
      ],
      "metadata": {
        "id": "vaMPlLyQjkxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "filename = 'final_model.pkl'\n",
        "pickle.dump(best_lr, open(filename, 'wb'))\n",
        "\n",
        "# Load the model from the pickle file\n"
      ],
      "metadata": {
        "id": "YhlXMwFltGe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('scaler.pkl', 'wb') as scaler_file:\n",
        "    pickle.dump(scaler, scaler_file)\n",
        "\n",
        "with open('label_encoder.pkl', 'wb') as encoder_file:\n",
        "    pickle.dump(label_encoder, encoder_file)\n",
        "\n",
        "# To load them later:\n",
        "# with open('scaler.pkl', 'rb') as scaler_file:\n",
        "#     loaded_scaler = pickle.load(scaler_file)\n",
        "\n",
        "# with open('label_encoder.pkl', 'rb') as encoder_file:\n",
        "#     loaded_encoder = pickle.load(encoder_file)"
      ],
      "metadata": {
        "id": "zWnquZasyKJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "y_pred = loaded_model.predict(X_test_scaled)\n",
        "\n",
        "accuracy_loaded = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of loaded model:\", accuracy_loaded)\n"
      ],
      "metadata": {
        "id": "kfBgiCQ2t-JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "new_user_data = pd.DataFrame({\n",
        "    'Temperature': [25.5],\n",
        "    'Humidity': [60.0],\n",
        "    'PM2.5': [15.0],\n",
        "    'PM10': [30.0],\n",
        "    'NO2': [10.0],\n",
        "    'SO2': [5.0],\n",
        "    'CO': [2.0],\n",
        "    'Proximity_to_Industrial_Areas': [0.1],\n",
        "    'Population_Density': [1500]\n",
        "})\n",
        "\n",
        "user_data_scaled = scaler.transform(new_user_data)\n",
        "\n",
        "# Predict the air quality for the new user data\n",
        "predicted_air_quality_encoded = loaded_model.predict(user_data_scaled)\n",
        "\n",
        "# Inverse transform the predicted value to get the original label\n",
        "predicted_air_quality_label = label_encoder.inverse_transform(predicted_air_quality_encoded)\n",
        "\n",
        "print(\"Predicted Air Quality for the new user:\", predicted_air_quality_label[0])\n"
      ],
      "metadata": {
        "id": "PXLJliIquDgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rf= RandomForestClassifier()\n",
        "# rf.fit(X_train_scaled,y_train)\n",
        "# y_pred=rf.predict(X_test_scaled)\n",
        "# accuracy_score(y_test,y_pred)\n",
        "# 0.957"
      ],
      "metadata": {
        "id": "zSO6VlbsZvy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train_pred_lr = lr.predict(X_train_scaled)\n",
        "\n",
        "print(f\"Random Forest Training Accuracy: {accuracy_score(y_train, y_train_pred_lr)}\")\n",
        "y_test_pred_rf = lr.predict(X_test_scaled)\n",
        "\n",
        "print(f\"Random Forest Testing Accuracy: {accuracy_score(y_test, y_test_pred_rf)}\")\n"
      ],
      "metadata": {
        "id": "B5qxMHtMZ4Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***HYPER PARAMETER TUNING***"
      ],
      "metadata": {
        "id": "i11wT1-gaajp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LETS TRY TO USE FCNN FOR BETTER ACCURACY"
      ],
      "metadata": {
        "id": "lmprVloSboqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna tensorflow scikeras"
      ],
      "metadata": {
        "id": "yRP6_phxdsw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import optuna\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam # Import Adam optimizer\n",
        "import numpy as np # Import numpy for flattening\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define the build_model function within this cell\n",
        "def build_model(optimizers='adam', dropout_rate=0.2, units=64):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(units=units, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(units=units, activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(units=units, activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "\n",
        "  model.add(Dense(units=len(y_train.unique()), activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=optimizers, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to tune\n",
        "    optimizers = trial.suggest_categorical('optimizers', ['adam', 'rmsprop'])\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    units = trial.suggest_categorical('units', [32, 64, 128])\n",
        "    # Add learning rate to tune\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
        "\n",
        "    model = build_model(optimizers=optimizers, dropout_rate=dropout_rate, units=units)\n",
        "    if optimizers == 'adam':\n",
        "        optimizer_instance = Adam(learning_rate=learning_rate)\n",
        "    else: # rmsprop\n",
        "        optimizer_instance = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) # Use RMSprop\n",
        "\n",
        "    model.compile(optimizer=optimizer_instance, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    keras_classifier = KerasClassifier(model=model, epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "\n",
        "    keras_classifier.fit(X_train_scaled, y_train.values.flatten())\n",
        "    # Flatten y_test before scoring\n",
        "    accuracy = keras_classifier.score(X_test_scaled, y_test.values.flatten())\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Create an Optuna study and optimize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Print the best parameters and the best accuracy\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Value: {trial.value}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# You can now build the final model with the best parameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Build the best model with the found parameters\n",
        "best_model = build_model(optimizers=best_params['optimizers'],\n",
        "                         dropout_rate=best_params['dropout_rate'],\n",
        "                         units=best_params['units'])\n",
        "\n",
        "# Compile the best model with the best learning rate\n",
        "if best_params['optimizers'] == 'adam':\n",
        "    best_optimizer = Adam(learning_rate=best_params['learning_rate'])\n",
        "else:\n",
        "    best_optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_params['learning_rate'])\n",
        "\n",
        "best_model.compile(optimizer=best_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the best model on the full training data\n",
        "# Flatten y_train before fitting the best model\n",
        "best_model.fit(X_train_scaled, y_train.values.flatten(), epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "# Flatten y_test before evaluating\n",
        "loss, accuracy = best_model.evaluate(X_test_scaled, y_test.values.flatten(), verbose=0)\n",
        "print(f\"Accuracy of the best model on test data: {accuracy}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2bobiEpvdYs0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}